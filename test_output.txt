Loading environment from: /Users/jj/rambo2/env
Starting end-to-end test...
Calling https://ghpyultwdcrdduksujxi.supabase.co/functions/v1/process-sred...
Response received!
Narrative generated (first 100 chars): ## Line 242: Technological Uncertainty

We faced a technological uncertainty in computer vision: whe...
❌ No PDF URL in response
Full response: {
  "result": "## Line 242: Technological Uncertainty\n\nWe faced a technological uncertainty in computer vision: whether a model could simultaneously achieve sub-100 ms end-to-end inference latency and ≥99% detection accuracy for micro-defects smaller than 0.1 mm under real manufacturing conditions (variable illumination, motion blur, specular reflections) on constrained edge compute. Publicly available methods exhibited a speed–accuracy trade-off we could not resolve through routine tuning:\n- Off-the-shelf CNN detectors/classifiers (e.g., ResNet/EfficientNet backbones with FPN/UNet heads, YOLO variants) either met latency but missed micro-defects due to loss of fine-texture cues at downsampled scales, or detected them but exceeded ~200 ms latency.\n- Transformer models improved global context but were computationally heavy; naïve self-attention exceeded edge latency budgets.\n- Classical CV (DoG/Laplacian, morphology) was brittle to lighting and texture variation and produced unacceptable false positives on brushed metals.\n\nWhat was not known was how to architect and deploy a hybrid CNN–Transformer model with an attention mechanism that preserved sub-pixel texture sensitivity while remaining computationally feasible for real-time inspection. The uncertainty was technological (architecture-level constraints, attention design, quantization effects on micro-texture features), not a business goal (e.g., “reduce scrap rate”). Standard practice (hyper-parameter tuning, larger datasets, or hardware upgrades) was insufficient because the bottleneck arose from fundamental interactions between receptive field, feature map resolution, attention complexity, and deployment-time operator efficiency on the target edge runtime. No authoritative guidance or benchmarks existed for <0.1 mm defect detection at <100 ms in our operating envelope, making the solution non-obvious and not solvable via routine engineering.\n\n## Line 244: Systematic Investigation  \n\nSR&ED Hypothesis: A hybrid CNN–Transformer with windowed/sparse local-global attention and multi-scale feature fusion, combined with deployment optimizations (operator fusion and INT8 calibration), would achieve <100 ms end-to-end latency and ≥99% accuracy for <0.1 mm defects on our inspection line.\n\nWe ran controlled experiments on 10,000 labeled images (balanced across micro-defect types; augmentations: motion blur, illumination jitter, Gaussian noise). We benchmarked five architectures end-to-end (pre/post-processing included) on the production edge runtime:\n\n- A) Baseline CNN (EfficientNet-B3 + FPN). Result: 200–215 ms; recall for <0.1 mm = 96.8%.\n- B) CNN + deformable conv + deeper FPN. Result: 165 ms; 97.9% recall; marginal gains, still slow.\n- C) Hybrid: CNN stem + windowed self-attention (Swin-like) with sparse attention, cross-scale fusion, lightweight attention heads; deployment with ONNX/TensorRT, layer fusion, per-channel INT8 calibration. Result: 92 ms; 99.2% accuracy (AUROC 0.997); recall for <0.1 mm = 99.1%.\n- D) Pure ViT (Linformer). Result: 160–180 ms; 98.0% accuracy; unacceptable latency.\n- E) YOLOv5m with high-res input and SIoU loss. Result: 110–125 ms; 97.5% recall; missed micro-defects due to aliasing at stride-32.\n\nAblations (logged in experiment tracker and Jira tickets tagged “SR&ED-Experiment”): attention window size (7/11/15), number of heads, fusion depth, quantization schemes (per-tensor vs per-channel). We observed INT8 per-tensor degraded micro-texture sensitivity (−0.8% recall); per-channel restored it while holding latency. Profiling identified non-max suppression as a hotspot; replacing standard NMS with batched NMS shaved ~8 ms. Evidence: Git commits referencing model variants (e.g., “hybrid-attn-win11-int8”), benchmark scripts with dated CSV outputs, weekly meeting notes summarizing metrics, and timesheets allocating hours to experimental tasks. Negative results (A, D, E) informed subsequent iterations.\n\n## Line 246: Technological Advancement\n\nWe generated new knowledge about constructing and deploying transformer-enhanced vision models for real-time micro-defect detection on constrained edge systems:\n\n- Demonstrated that local windowed/sparse attention integrated at intermediate feature scales, coupled with cross-scale fusion, preserves sub-pixel texture cues critical for <0.1 mm defects while keeping attention complexity tractable. Pure CNNs under-fit micro-texture at acceptable speeds; pure ViT overran latency.\n- Quantified the interaction between feature-map resolution and detectability: maintaining an effective receptive field with ≥0.05 mm/px at the fusion stage was necessary to hit ≥99% recall; higher downsampling irreversibly lost cues.\n- Showed that INT8 quantization is viable for micro-defect sensitivity only with per-channel calibration and operator fusion; per-tensor quantization disproportionately suppressed fine-gradient responses.\n- Identified deployment bottlenecks (post-processing NMS) and provided a method (batched NMS + operator fusion) to reduce latency without degrading accuracy.\n\nThese results are technological, not merely product features. They clarify how to balance receptive field, attention scope, and deployment optimizations to meet simultaneous constraints (speed and micro-defect sensitivity) that standard literature and frameworks do not address. Negative findings—e.g., YOLO variants missing <0.1 mm defects despite high-resolution input, and pure ViT latency exceeding limits—define boundaries that guide future designs.\n\nSR&ED vs routine: Experimental architecture design, ablations, quantization studies, and profiling were SR&ED. Non-eligible work (UI/HMI, PLC integration, camera enclosure, general refactoring, routine dataset labeling beyond the experimental subset, and production rollout) was excluded. Contemporaneous documentation (tickets, commits, benchmark logs, timesheets) substantiated the systematic investigation."
}
